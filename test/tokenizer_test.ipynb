{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentences': [{'text': {'content': '안녕하세요.', 'length': 6, 'beginOffset': 0}, 'tokens': [{'text': {'content': '안녕하세요.', 'length': 6, 'beginOffset': 0}, 'segments': [{'text': {'content': '안녕', 'length': 2, 'beginOffset': 0}, 'hint': 'N'}, {'text': {'content': '하', 'beginOffset': 2, 'length': 1}, 'hint': 'V'}, {'text': {'content': '시', 'beginOffset': 3, 'length': 1}, 'hint': 'E'}, {'text': {'content': '어요', 'beginOffset': 3, 'length': 2}, 'hint': 'E'}, {'text': {'content': '.', 'beginOffset': 5, 'length': 1}, 'hint': 'S'}], 'tagged': '안녕/N+하/V+시/E+어요/E+./S'}]}, {'text': {'content': '반가워요!', 'beginOffset': 7, 'length': 5}, 'tokens': [{'text': {'content': '반가워요!', 'beginOffset': 7, 'length': 5}, 'segments': [{'text': {'content': '반갑', 'beginOffset': 7, 'length': 2}, 'hint': 'V'}, {'text': {'content': '어요', 'beginOffset': 9, 'length': 2}, 'hint': 'E'}, {'text': {'content': '!', 'beginOffset': 11, 'length': 1}, 'hint': 'S'}], 'tagged': '반갑/V+어요/E+!/S'}]}], 'language': 'ko_KR'}\n",
      "['안녕', '하', '시', '어요', '.', '반갑', '어요', '!']\n",
      "['안녕']\n",
      "['하', '반갑']\n",
      "[]\n",
      "[]\n",
      "['.', '!']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import google.protobuf.text_format as tf\n",
    "from bareunpy import Tokenizer\n",
    "\n",
    "# If you have your own localhost bareun.\n",
    "tokenizer = Tokenizer('175.106.88.249',5757)\n",
    "# or if you have your own bareun which is running on 10.8.3.211:15656.\n",
    "# my_tokenizer = Tagger('175.106.88.249',5757)\n",
    "# or with smaller public cloud instance, it may be slow. It is free.\n",
    "# tokenizer = Tokenizer()\n",
    "\n",
    "# print results.\n",
    "tokenized = tokenizer.tokenize_list([\"안녕하세요.\", \"반가워요!\"])\n",
    "# print(tokenized)\n",
    "# get protobuf message.\n",
    "# m = tokenized.msg()\n",
    "# print(tokenized.segments())\n",
    "# print(m.sentences[0].tokens[0].segments[0])\n",
    "\n",
    "\n",
    "# get json object\n",
    "jo = tokenized.as_json()\n",
    "print(jo)\n",
    "\n",
    "# get tuple of segments\n",
    "ss = tokenized.segments()\n",
    "print(ss)\n",
    "ns = tokenized.nouns()\n",
    "print(ns)\n",
    "vs = tokenized.verbs()\n",
    "print(vs)\n",
    "# postpositions: 조사\n",
    "ps = tokenized.postpositions()\n",
    "print(ps)\n",
    "# Adverbs, 부사\n",
    "ass = tokenized.adverbs()\n",
    "print(ass)\n",
    "ss = tokenized.symbols()\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('안녕하', 'VA'), ('시', 'EP'), ('어요', 'EF'), ('반갑', 'VA'), ('어요', 'EF')]\n"
     ]
    }
   ],
   "source": [
    "from bareunpy import Tagger\n",
    "\n",
    "tagger = Tagger('175.106.88.249',5757)\n",
    "text = '안녕하세요 반가워요'\n",
    "\n",
    "pos = tagger.pos(text)\n",
    "res = tagger.tags(text)\n",
    "\n",
    "print(pos)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
